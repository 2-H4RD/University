{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLOWimsEYK8O",
        "outputId": "c074fd5d-2291-486b-f0fc-c38ab471e036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.932 0.936\n"
          ]
        }
      ],
      "source": [
        "#Практика 3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "def logistic_gradient_descent(X, y, reg=0, k=0.1, eps=1e-5, max_iter=10000):\n",
        "    m, n = X.shape\n",
        "    X_with_bias = np.c_[np.ones(m), X]  # (m, n+1)\n",
        "    w = np.zeros(n + 1)\n",
        "    for i in range(max_iter):\n",
        "        z = X_with_bias.dot(w)  # (m,)\n",
        "        yz = y * z\n",
        "        p = sigmoid(-yz)\n",
        "        grad = -1.0 / m * (X_with_bias.T.dot(y * p))  # (n+1,)\n",
        "        if reg > 0:\n",
        "            grad[1:] += reg * w[1:]\n",
        "        new_w = w - k * grad\n",
        "        if np.linalg.norm(new_w - w) < eps:\n",
        "            w = new_w\n",
        "            break\n",
        "        w = new_w\n",
        "    return w\n",
        "def predict_proba(X, w):\n",
        "    X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
        "    z = X_with_bias.dot(w)\n",
        "    return sigmoid(z)\n",
        "\n",
        "data = pd.read_csv('data-logistic.csv', header=None)\n",
        "y = data.iloc[:, 0].values\n",
        "X = data.iloc[:, [1, 2]].values\n",
        "w1 = logistic_gradient_descent(X, y, reg=0)\n",
        "w2 = logistic_gradient_descent(X, y, reg=10)\n",
        "proba1 = predict_proba(X, w1)\n",
        "proba2 = predict_proba(X, w2)\n",
        "auc1 = roc_auc_score(y, proba1)\n",
        "auc2 = roc_auc_score(y, proba2)\n",
        "print(f\"{auc1:.3f} {auc2:.3f}\")\n",
        "#Более длинные шаги: могут привести к расходимости.\n",
        "#Меньшие шаги: увеличивают число итераций до сходимости.\n",
        "#Изменение начального приближения: для выпуклой задачи (как логистическая регрессия) результат не должен зависеть от начального приближения, если сходится."
      ]
    }
  ]
}